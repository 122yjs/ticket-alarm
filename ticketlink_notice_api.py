import requests, time, json
from bs4 import BeautifulSoup
from datetime import datetime
import schedule, os
import requests
import time
import json

BASE = "https://www.ticketlink.co.kr"
LIST_API = f"{BASE}/help/getNoticeList"          # JSì—ì„œ í˜¸ì¶œí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸[2]

sess = requests.Session()
sess.headers.update({
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/124.0 Safari/537.36"),
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Referer": f"{BASE}/help/notice"
})

def fetch_list(page=1, category="", keyword=""):
    try:
        print(f"ğŸ“‹ ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­ ì¤‘... (í˜ì´ì§€: {page})")
        res = sess.get(LIST_API,
                       params={"page": page,
                               "noticeCategoryCode": category,
                               "title": keyword.replace(" ", "") if keyword else None},
                       timeout=10).json()
        items = res["result"]["result"]
        print(f"ğŸ“‹ {len(items)}ê°œì˜ ê³µì§€ì‚¬í•­ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return items
    except Exception as e:
        print(f"âœ— ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        return []

def fetch_detail(nid):
    url = f"{BASE}/help/notice/{nid}"
    try:
        html = sess.get(url, timeout=10).text
        soup = BeautifulSoup(html, "html.parser")
        
        # ì œëª© ì°¾ê¸° - ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ëŠ” ì„ íƒì ì‚¬ìš©
        title_element = soup.select_one('dd.title') or soup.select_one('#noticeTitle') or soup.select_one('h2')
        title = title_element.get_text(strip=True) if title_element else f"ì œëª© ì—†ìŒ (ID: {nid})"

        # ë³¸ë¬¸ ì°¾ê¸° - ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ëŠ” ì„ íƒì ì‚¬ìš©
        body_element = soup.select_one('dd.list_cont') or soup.select_one('.list_cont') or soup.select_one('.content')
        body = body_element.get_text("\n", strip=True) if body_element else "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"âœ“ ê³µì§€ì‚¬í•­ {nid} ìˆ˜ì§‘ ì™„ë£Œ: {title[:50]}...")
        return {"id": nid, "title": title, "body": body, "url": url}
        
    except Exception as e:
        print(f"âœ— ê³µì§€ì‚¬í•­ {nid} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        return {"id": nid, "title": f"ì˜¤ë¥˜ (ID: {nid})", "body": f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}", "url": url}

def job():
    print(f"\nğŸš€ í‹°ì¼“ë§í¬ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    items = fetch_list(page=1, category="", keyword="")
    if not items:
        print("âŒ ìˆ˜ì§‘í•  ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    saved = []
    total = len(items)
    
    for i, it in enumerate(items, 1):
        print(f"\n[{i}/{total}] ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ ì¤‘...")
        d = fetch_detail(it["noticeId"])
        saved.append(d)
        time.sleep(0.3)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
    
    # ê²°ê³¼ ì €ì¥
    today = datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs("data", exist_ok=True)
    filename = f"data/notice_{today}.json"
    
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(saved, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! {len(saved)}ê±´ì˜ ê³µì§€ì‚¬í•­ì„ '{filename}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    print(f"â° ë‹¤ìŒ ìˆ˜ì§‘ì€ 10ë¶„ í›„ì— ì‹¤í–‰ë©ë‹ˆë‹¤.\n")

if __name__ == "__main__":
    # 10ë¶„ë§ˆë‹¤ ì‹¤í–‰
    schedule.every(10).minutes.do(job)                           # ìë™ ìŠ¤ì¼€ì¤„[6]
    job()                                                        # ì²« ì‹¤í–‰
    while True:
        schedule.run_pending()
        time.sleep(1)
