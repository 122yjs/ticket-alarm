#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í‹°ì¼“ ì•Œë¦¼ ì‹œìŠ¤í…œ ë¡œê·¸ ë¶„ì„ê¸°
ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´, ì˜¤ë¥˜, ì„±ëŠ¥ ì´ìŠˆë¥¼ íƒì§€í•©ë‹ˆë‹¤.
"""

import os
import re
import json
import sqlite3
import argparse
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from pathlib import Path
import logging
import gzip
import requests

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class LogAnalyzer:
    """ë¡œê·¸ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, config_file='config.json'):
        self.config = self.load_config(config_file)
        self.db_path = 'log_analysis.db'
        self.init_database()
        
        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        self.log_paths = {
            'app': 'logs/ticket_alarm.log',
            'crawler': 'logs/crawler.log',
            'discord': 'logs/discord.log',
            'error': 'logs/error.log',
            'access': 'logs/access.log',
            'performance': 'logs/performance_monitor.log'
        }
        
        # ë¡œê·¸ íŒ¨í„´ ì •ì˜
        self.patterns = {
            'error': re.compile(r'ERROR|CRITICAL|Exception|Traceback', re.IGNORECASE),
            'warning': re.compile(r'WARNING|WARN', re.IGNORECASE),
            'success': re.compile(r'SUCCESS|ì™„ë£Œ|ì„±ê³µ', re.IGNORECASE),
            'failed': re.compile(r'FAILED|ì‹¤íŒ¨|ì˜¤ë¥˜', re.IGNORECASE),
            'timeout': re.compile(r'timeout|ì‹œê°„ì´ˆê³¼', re.IGNORECASE),
            'connection': re.compile(r'connection|ì—°ê²°', re.IGNORECASE),
            'memory': re.compile(r'memory|ë©”ëª¨ë¦¬|OutOfMemory', re.IGNORECASE),
            'disk': re.compile(r'disk|ë””ìŠ¤í¬|No space', re.IGNORECASE),
            'network': re.compile(r'network|ë„¤íŠ¸ì›Œí¬|DNS', re.IGNORECASE),
            'permission': re.compile(r'permission|ê¶Œí•œ|denied', re.IGNORECASE),
            'rate_limit': re.compile(r'rate.?limit|ì†ë„.?ì œí•œ', re.IGNORECASE),
            'webhook': re.compile(r'webhook|ì›¹í›…', re.IGNORECASE),
            'crawling': re.compile(r'crawl|í¬ë¡¤ë§|scraping', re.IGNORECASE),
            'notification': re.compile(r'notification|ì•Œë¦¼|alert', re.IGNORECASE)
        }
        
        # ì‹œê°„ íŒ¨í„´
        self.time_patterns = [
            re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})'),  # YYYY-MM-DD HH:MM:SS
            re.compile(r'(\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})'),  # MM/DD/YYYY HH:MM:SS
            re.compile(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})'),  # ISO format
        ]
    
    def load_config(self, config_file):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def init_database(self):
        """ë¡œê·¸ ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ë¡œê·¸ ì—”íŠ¸ë¦¬ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS log_entries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME,
                    log_type TEXT,
                    level TEXT,
                    message TEXT,
                    file_path TEXT,
                    line_number INTEGER,
                    pattern_matches TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ë¡œê·¸ í†µê³„ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS log_statistics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date DATE,
                    log_type TEXT,
                    level TEXT,
                    count INTEGER,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # íŒ¨í„´ ë¶„ì„ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pattern_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date DATE,
                    pattern_name TEXT,
                    count INTEGER,
                    sample_messages TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ì•Œë¦¼ ì´ë ¥ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS analysis_alerts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    alert_type TEXT,
                    severity TEXT,
                    message TEXT,
                    details TEXT
                )
            ''')
            
            conn.commit()
            conn.close()
            
            logger.info("ë¡œê·¸ ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def parse_log_line(self, line, log_type):
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        try:
            # ì‹œê°„ ì¶”ì¶œ
            timestamp = None
            for pattern in self.time_patterns:
                match = pattern.search(line)
                if match:
                    time_str = match.group(1)
                    try:
                        # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%m/%d/%Y %H:%M:%S', '%Y-%m-%dT%H:%M:%S']:
                            try:
                                timestamp = datetime.strptime(time_str, fmt)
                                break
                            except ValueError:
                                continue
                        break
                    except ValueError:
                        pass
            
            # ë¡œê·¸ ë ˆë²¨ ì¶”ì¶œ
            level = 'INFO'  # ê¸°ë³¸ê°’
            level_patterns = {
                'DEBUG': re.compile(r'DEBUG', re.IGNORECASE),
                'INFO': re.compile(r'INFO', re.IGNORECASE),
                'WARNING': re.compile(r'WARNING|WARN', re.IGNORECASE),
                'ERROR': re.compile(r'ERROR', re.IGNORECASE),
                'CRITICAL': re.compile(r'CRITICAL|FATAL', re.IGNORECASE)
            }
            
            for level_name, pattern in level_patterns.items():
                if pattern.search(line):
                    level = level_name
                    break
            
            # íŒ¨í„´ ë§¤ì¹­
            matched_patterns = []
            for pattern_name, pattern in self.patterns.items():
                if pattern.search(line):
                    matched_patterns.append(pattern_name)
            
            return {
                'timestamp': timestamp or datetime.now(),
                'log_type': log_type,
                'level': level,
                'message': line.strip(),
                'pattern_matches': ','.join(matched_patterns)
            }
            
        except Exception as e:
            logger.error(f"ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_log_file(self, file_path, log_type):
        """ë¡œê·¸ íŒŒì¼ ë¶„ì„"""
        if not os.path.exists(file_path):
            logger.warning(f"ë¡œê·¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return []
        
        entries = []
        line_number = 0
        
        try:
            # gzip íŒŒì¼ ì²˜ë¦¬
            if file_path.endswith('.gz'):
                file_obj = gzip.open(file_path, 'rt', encoding='utf-8')
            else:
                file_obj = open(file_path, 'r', encoding='utf-8')
            
            with file_obj as f:
                for line in f:
                    line_number += 1
                    
                    if not line.strip():
                        continue
                    
                    entry = self.parse_log_line(line, log_type)
                    if entry:
                        entry['file_path'] = file_path
                        entry['line_number'] = line_number
                        entries.append(entry)
            
            logger.info(f"{log_type} ë¡œê·¸ ë¶„ì„ ì™„ë£Œ: {len(entries)}ê°œ ì—”íŠ¸ë¦¬")
            return entries
            
        except Exception as e:
            logger.error(f"ë¡œê·¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return []
    
    def save_log_entries(self, entries):
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not entries:
            return
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for entry in entries:
                cursor.execute('''
                    INSERT INTO log_entries (
                        timestamp, log_type, level, message, 
                        file_path, line_number, pattern_matches
                    ) VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    entry['timestamp'],
                    entry['log_type'],
                    entry['level'],
                    entry['message'],
                    entry['file_path'],
                    entry['line_number'],
                    entry['pattern_matches']
                ))
            
            conn.commit()
            conn.close()
            
            logger.info(f"{len(entries)}ê°œ ë¡œê·¸ ì—”íŠ¸ë¦¬ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¡œê·¸ ì—”íŠ¸ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def generate_statistics(self, date=None):
        """ë¡œê·¸ í†µê³„ ìƒì„±"""
        if date is None:
            date = datetime.now().date()
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ê¸°ì¡´ í†µê³„ ì‚­ì œ
            cursor.execute('DELETE FROM log_statistics WHERE date = ?', (date,))
            
            # ë¡œê·¸ íƒ€ì…ë³„, ë ˆë²¨ë³„ í†µê³„
            cursor.execute('''
                SELECT log_type, level, COUNT(*) as count
                FROM log_entries 
                WHERE DATE(timestamp) = ?
                GROUP BY log_type, level
            ''', (date,))
            
            stats = cursor.fetchall()
            
            for log_type, level, count in stats:
                cursor.execute('''
                    INSERT INTO log_statistics (date, log_type, level, count)
                    VALUES (?, ?, ?, ?)
                ''', (date, log_type, level, count))
            
            conn.commit()
            conn.close()
            
            logger.info(f"{date} ë¡œê·¸ í†µê³„ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¡œê·¸ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def analyze_patterns(self, date=None, days=1):
        """íŒ¨í„´ ë¶„ì„"""
        if date is None:
            date = datetime.now().date()
        
        start_date = date - timedelta(days=days-1)
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ê¸°ì¡´ íŒ¨í„´ ë¶„ì„ ì‚­ì œ
            cursor.execute('DELETE FROM pattern_analysis WHERE date = ?', (date,))
            
            # íŒ¨í„´ë³„ í†µê³„
            cursor.execute('''
                SELECT pattern_matches, message
                FROM log_entries 
                WHERE DATE(timestamp) BETWEEN ? AND ?
                AND pattern_matches != ""
            ''', (start_date, date))
            
            results = cursor.fetchall()
            pattern_stats = defaultdict(list)
            
            for pattern_matches, message in results:
                for pattern in pattern_matches.split(','):
                    if pattern.strip():
                        pattern_stats[pattern.strip()].append(message)
            
            # íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ì €ì¥
            for pattern_name, messages in pattern_stats.items():
                count = len(messages)
                sample_messages = json.dumps(messages[:5])  # ìƒ˜í”Œ 5ê°œë§Œ ì €ì¥
                
                cursor.execute('''
                    INSERT INTO pattern_analysis (date, pattern_name, count, sample_messages)
                    VALUES (?, ?, ?, ?)
                ''', (date, pattern_name, count, sample_messages))
            
            conn.commit()
            conn.close()
            
            logger.info(f"{date} íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: {len(pattern_stats)}ê°œ íŒ¨í„´")
            return pattern_stats
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def detect_anomalies(self, date=None):
        """ì´ìƒ ì§•í›„ íƒì§€"""
        if date is None:
            date = datetime.now().date()
        
        anomalies = []
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ì˜¤ëŠ˜ê³¼ ì–´ì œ ë¹„êµ
            yesterday = date - timedelta(days=1)
            
            # ì—ëŸ¬ ê¸‰ì¦ íƒì§€
            cursor.execute('''
                SELECT 
                    (SELECT COUNT(*) FROM log_entries 
                     WHERE DATE(timestamp) = ? AND level IN ('ERROR', 'CRITICAL')) as today_errors,
                    (SELECT COUNT(*) FROM log_entries 
                     WHERE DATE(timestamp) = ? AND level IN ('ERROR', 'CRITICAL')) as yesterday_errors
            ''', (date, yesterday))
            
            result = cursor.fetchone()
            if result:
                today_errors, yesterday_errors = result
                if yesterday_errors > 0 and today_errors > yesterday_errors * 2:
                    anomalies.append({
                        'type': 'error_spike',
                        'severity': 'warning',
                        'message': f"ì˜¤ë¥˜ ê¸‰ì¦ íƒì§€: ì˜¤ëŠ˜ {today_errors}ê°œ, ì–´ì œ {yesterday_errors}ê°œ",
                        'details': {'today': today_errors, 'yesterday': yesterday_errors}
                    })
            
            # íŠ¹ì • íŒ¨í„´ ê¸‰ì¦ íƒì§€
            critical_patterns = ['timeout', 'connection', 'memory', 'disk']
            
            for pattern in critical_patterns:
                cursor.execute('''
                    SELECT 
                        (SELECT COALESCE(count, 0) FROM pattern_analysis 
                         WHERE date = ? AND pattern_name = ?) as today_count,
                        (SELECT COALESCE(count, 0) FROM pattern_analysis 
                         WHERE date = ? AND pattern_name = ?) as yesterday_count
                ''', (date, pattern, yesterday, pattern))
                
                result = cursor.fetchone()
                if result:
                    today_count, yesterday_count = result
                    if yesterday_count > 0 and today_count > yesterday_count * 3:
                        anomalies.append({
                            'type': 'pattern_spike',
                            'severity': 'warning',
                            'message': f"{pattern} íŒ¨í„´ ê¸‰ì¦: ì˜¤ëŠ˜ {today_count}ê°œ, ì–´ì œ {yesterday_count}ê°œ",
                            'details': {'pattern': pattern, 'today': today_count, 'yesterday': yesterday_count}
                        })
            
            # ë¡œê·¸ ë¶€ì¬ íƒì§€ (í¬ë¡¤ë§ ì¤‘ë‹¨ ë“±)
            cursor.execute('''
                SELECT log_type, COUNT(*) as count
                FROM log_entries 
                WHERE DATE(timestamp) = ?
                GROUP BY log_type
            ''', (date,))
            
            log_counts = dict(cursor.fetchall())
            expected_logs = ['app', 'crawler', 'discord']
            
            for log_type in expected_logs:
                if log_counts.get(log_type, 0) < 10:  # í•˜ë£¨ì— 10ê°œ ë¯¸ë§Œì´ë©´ ì´ìƒ
                    anomalies.append({
                        'type': 'log_absence',
                        'severity': 'critical',
                        'message': f"{log_type} ë¡œê·¸ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤: {log_counts.get(log_type, 0)}ê°œ",
                        'details': {'log_type': log_type, 'count': log_counts.get(log_type, 0)}
                    })
            
            conn.close()
            
            # ì´ìƒ ì§•í›„ ì €ì¥
            if anomalies:
                self.save_anomalies(anomalies)
            
            return anomalies
            
        except Exception as e:
            logger.error(f"ì´ìƒ ì§•í›„ íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def save_anomalies(self, anomalies):
        """ì´ìƒ ì§•í›„ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for anomaly in anomalies:
                cursor.execute('''
                    INSERT INTO analysis_alerts (alert_type, severity, message, details)
                    VALUES (?, ?, ?, ?)
                ''', (
                    anomaly['type'],
                    anomaly['severity'],
                    anomaly['message'],
                    json.dumps(anomaly['details'])
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ì´ìƒ ì§•í›„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def send_analysis_report(self, date=None):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ë°œì†¡"""
        if date is None:
            date = datetime.now().date()
        
        try:
            # í†µê³„ ì¡°íšŒ
            stats = self.get_daily_statistics(date)
            patterns = self.get_pattern_analysis(date)
            anomalies = self.detect_anomalies(date)
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = self.generate_report(date, stats, patterns, anomalies)
            
            # ë””ìŠ¤ì½”ë“œ ì›¹í›…ìœ¼ë¡œ ë°œì†¡
            webhook_url = self.config.get('DISCORD_WEBHOOK_URL')
            if webhook_url:
                self.send_discord_report(webhook_url, report)
            
            logger.info(f"{date} ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ ë°œì†¡ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ë¦¬í¬íŠ¸ ë°œì†¡ ì‹¤íŒ¨: {e}")
    
    def get_daily_statistics(self, date):
        """ì¼ì¼ í†µê³„ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT log_type, level, count
                FROM log_statistics 
                WHERE date = ?
                ORDER BY log_type, level
            ''', (date,))
            
            results = cursor.fetchall()
            conn.close()
            
            stats = defaultdict(dict)
            for log_type, level, count in results:
                stats[log_type][level] = count
            
            return dict(stats)
            
        except Exception as e:
            logger.error(f"ì¼ì¼ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_pattern_analysis(self, date):
        """íŒ¨í„´ ë¶„ì„ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT pattern_name, count, sample_messages
                FROM pattern_analysis 
                WHERE date = ?
                ORDER BY count DESC
                LIMIT 10
            ''', (date,))
            
            results = cursor.fetchall()
            conn.close()
            
            patterns = []
            for pattern_name, count, sample_messages in results:
                patterns.append({
                    'name': pattern_name,
                    'count': count,
                    'samples': json.loads(sample_messages)
                })
            
            return patterns
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ë¶„ì„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_report(self, date, stats, patterns, anomalies):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'date': date.strftime('%Y-%m-%d'),
            'summary': {
                'total_logs': sum(sum(levels.values()) for levels in stats.values()),
                'error_count': sum(stats.get(log_type, {}).get('ERROR', 0) + 
                                 stats.get(log_type, {}).get('CRITICAL', 0) 
                                 for log_type in stats),
                'warning_count': sum(stats.get(log_type, {}).get('WARNING', 0) 
                                   for log_type in stats),
                'anomaly_count': len(anomalies)
            },
            'statistics': stats,
            'top_patterns': patterns[:5],
            'anomalies': anomalies
        }
        
        return report
    
    def send_discord_report(self, webhook_url, report):
        """ë””ìŠ¤ì½”ë“œë¡œ ë¶„ì„ ë¦¬í¬íŠ¸ ë°œì†¡"""
        try:
            # ì‹¬ê°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
            color = 0x3498db  # ê¸°ë³¸ íŒŒë€ìƒ‰
            if report['summary']['anomaly_count'] > 0:
                color = 0xf39c12  # ì£¼í™©ìƒ‰
            if report['summary']['error_count'] > 50:
                color = 0xe74c3c  # ë¹¨ê°„ìƒ‰
            
            # ì„ë² ë“œ ìƒì„±
            embed = {
                'title': f"ğŸ“Š ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ - {report['date']}",
                'color': color,
                'fields': [
                    {
                        'name': 'ğŸ“ˆ ìš”ì•½',
                        'value': f"ì´ ë¡œê·¸: {report['summary']['total_logs']:,}ê°œ\n"
                                f"ì˜¤ë¥˜: {report['summary']['error_count']:,}ê°œ\n"
                                f"ê²½ê³ : {report['summary']['warning_count']:,}ê°œ\n"
                                f"ì´ìƒì§•í›„: {report['summary']['anomaly_count']}ê°œ",
                        'inline': True
                    }
                ],
                'timestamp': datetime.now().isoformat(),
                'footer': {
                    'text': 'í‹°ì¼“ ì•Œë¦¼ ì‹œìŠ¤í…œ ë¡œê·¸ ë¶„ì„ê¸°'
                }
            }
            
            # ì£¼ìš” íŒ¨í„´ ì¶”ê°€
            if report['top_patterns']:
                pattern_text = '\n'.join([
                    f"â€¢ {p['name']}: {p['count']}íšŒ"
                    for p in report['top_patterns'][:3]
                ])
                embed['fields'].append({
                    'name': 'ğŸ” ì£¼ìš” íŒ¨í„´',
                    'value': pattern_text,
                    'inline': True
                })
            
            # ì´ìƒ ì§•í›„ ì¶”ê°€
            if report['anomalies']:
                anomaly_text = '\n'.join([
                    f"âš ï¸ {a['message']}"
                    for a in report['anomalies'][:3]
                ])
                embed['fields'].append({
                    'name': 'ğŸš¨ ì´ìƒ ì§•í›„',
                    'value': anomaly_text,
                    'inline': False
                })
            
            payload = {
                'embeds': [embed],
                'username': 'ë¡œê·¸ ë¶„ì„ê¸°',
                'avatar_url': 'https://cdn-icons-png.flaticon.com/512/2920/2920277.png'
            }
            
            response = requests.post(
                webhook_url,
                json=payload,
                timeout=10
            )
            
            if response.status_code == 204:
                logger.info("ë””ìŠ¤ì½”ë“œ ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ ë°œì†¡ ì™„ë£Œ")
            else:
                logger.error(f"ë””ìŠ¤ì½”ë“œ ë¦¬í¬íŠ¸ ë°œì†¡ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"ë””ìŠ¤ì½”ë“œ ë¦¬í¬íŠ¸ ë°œì†¡ ì˜¤ë¥˜: {e}")
    
    def analyze_all_logs(self, days=1):
        """ëª¨ë“  ë¡œê·¸ íŒŒì¼ ë¶„ì„"""
        logger.info(f"ìµœê·¼ {days}ì¼ê°„ ë¡œê·¸ ë¶„ì„ ì‹œì‘")
        
        all_entries = []
        
        # ê° ë¡œê·¸ íŒŒì¼ ë¶„ì„
        for log_type, log_path in self.log_paths.items():
            if os.path.exists(log_path):
                entries = self.analyze_log_file(log_path, log_type)
                all_entries.extend(entries)
            
            # ë¡œí…Œì´ì…˜ëœ ë¡œê·¸ íŒŒì¼ë„ ë¶„ì„ (ìµœê·¼ ë©°ì¹ )
            for i in range(1, days + 1):
                rotated_path = f"{log_path}.{i}"
                if os.path.exists(rotated_path):
                    entries = self.analyze_log_file(rotated_path, log_type)
                    all_entries.extend(entries)
                
                # gzip ì••ì¶•ëœ ë¡œê·¸
                gz_path = f"{log_path}.{i}.gz"
                if os.path.exists(gz_path):
                    entries = self.analyze_log_file(gz_path, log_type)
                    all_entries.extend(entries)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        if all_entries:
            self.save_log_entries(all_entries)
        
        # í†µê³„ ë° íŒ¨í„´ ë¶„ì„
        today = datetime.now().date()
        self.generate_statistics(today)
        self.analyze_patterns(today, days)
        
        logger.info(f"ë¡œê·¸ ë¶„ì„ ì™„ë£Œ: {len(all_entries)}ê°œ ì—”íŠ¸ë¦¬ ì²˜ë¦¬")
        
        return all_entries
    
    def cleanup_old_data(self, days=30):
        """ì˜¤ë˜ëœ ë¶„ì„ ë°ì´í„° ì •ë¦¬"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cutoff_date = datetime.now() - timedelta(days=days)
            
            # ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ
            tables = ['log_entries', 'log_statistics', 'pattern_analysis', 'analysis_alerts']
            total_deleted = 0
            
            for table in tables:
                if table in ['log_entries', 'analysis_alerts']:
                    cursor.execute(f'DELETE FROM {table} WHERE timestamp < ?', (cutoff_date,))
                else:
                    cursor.execute(f'DELETE FROM {table} WHERE created_at < ?', (cutoff_date,))
                
                total_deleted += cursor.rowcount
            
            conn.commit()
            conn.close()
            
            if total_deleted > 0:
                logger.info(f"ì˜¤ë˜ëœ ë¡œê·¸ ë¶„ì„ ë°ì´í„° {total_deleted}ê°œ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='í‹°ì¼“ ì•Œë¦¼ ì‹œìŠ¤í…œ ë¡œê·¸ ë¶„ì„ê¸°')
    parser.add_argument('--config', '-c', default='config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--days', '-d', type=int, default=1, help='ë¶„ì„í•  ì¼ìˆ˜')
    parser.add_argument('--report', '-r', action='store_true', help='ë¶„ì„ ë¦¬í¬íŠ¸ ë°œì†¡')
    parser.add_argument('--cleanup', action='store_true', help='ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬')
    parser.add_argument('--anomalies', '-a', action='store_true', help='ì´ìƒ ì§•í›„ë§Œ íƒì§€')
    
    args = parser.parse_args()
    
    # ë¡œê·¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = LogAnalyzer(args.config)
    
    if args.cleanup:
        # ë°ì´í„° ì •ë¦¬
        analyzer.cleanup_old_data()
    elif args.anomalies:
        # ì´ìƒ ì§•í›„ íƒì§€ë§Œ
        anomalies = analyzer.detect_anomalies()
        if anomalies:
            print(f"\n=== ì´ìƒ ì§•í›„ íƒì§€ ê²°ê³¼ ===")
            for anomaly in anomalies:
                print(f"[{anomaly['severity'].upper()}] {anomaly['message']}")
        else:
            print("ì´ìƒ ì§•í›„ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        # ì „ì²´ ë¡œê·¸ ë¶„ì„
        analyzer.analyze_all_logs(args.days)
        
        if args.report:
            # ë¶„ì„ ë¦¬í¬íŠ¸ ë°œì†¡
            analyzer.send_analysis_report()
        
        # ì´ìƒ ì§•í›„ íƒì§€
        anomalies = analyzer.detect_anomalies()
        if anomalies:
            print(f"\nâš ï¸  {len(anomalies)}ê°œì˜ ì´ìƒ ì§•í›„ê°€ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for anomaly in anomalies:
                print(f"  [{anomaly['severity'].upper()}] {anomaly['message']}")

if __name__ == '__main__':
    main()